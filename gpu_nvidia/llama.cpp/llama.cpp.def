Bootstrap: docker
From: nvcr.io/nvidia/pytorch:24.05-py3

%setup
  mkdir ${SINGULARITY_ROOTFS}/lvs0

%post
  apt-get update && apt-get install -y --no-install-recommends \
      wget \
      vim \
      build-essential \
      libcurl4-openssl-dev \
      && apt-get clean && rm -rf /var/lib/apt/lists/*

  mkdir /opt/uv
  export UV_INSTALL_DIR=/opt/uv
  curl -LsSf https://astral.sh/uv/install.sh | sh

  cd /opt
  git clone https://github.com/ggerganov/llama.cpp.git
  cd llama.cpp
  git checkout b4953
  cmake -B build -DGGML_CUDA=ON -DGGML_CUDA_ENABLE_UNIFIED_MEMORY=1 -DGGML_CUDA_FORCE_CUBLAS=true -DGGML_CUDA_F16=true -DLLAMA_CURL=ON
  cd build
  cmake --build . --verbose -j32
  cmake --install .
  cd /opt
  rm -rf llama.cpp

  cd /opt
  export VERSION=1.22.6 OS=linux ARCH=arm64 && \
    wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \
    tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \
    rm go$VERSION.$OS-$ARCH.tar.gz

  cd /opt
  git clone https://github.com/ollama/ollama.git
  cd ollama
  cmake -B build
  cmake --build build --verbose -j32
  cmake --install build

%environment
  export PATH=/usr/local/cuda/bin:/opt/uv/bin:/usr/local/go/bin:$PATH
  export LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/lib:$LD_LIBRARY_PATH
  export NVIDIA_VISIBLE_DEVICES=all
  export NVIDIA_DRIVER_CAPABILITIES=compute,utility
