Bootstrap: docker
From: nvidia/cuda:12.8.0-devel-ubuntu22.04

%setup
  mkdir ${SINGULARITY_ROOTFS}/lvs0

%post
  apt-get update && apt-get install -y --no-install-recommends \
      wget \
      git \
      curl \
      ca-certificates \
      gnupg \
      pkg-config \
      libopenblas-dev \
      libssl-dev \
      vim \
      build-essential \
      libcurl4-openssl-dev \
      openjdk-17-jdk \
      maven \
      tesseract-ocr \
      tesseract-ocr-jpn \
      poppler-utils \
      swig \
      python3 \
      python3-pip \
      python3-dev \
      python3-venv \
      && apt-get clean && rm -rf /var/lib/apt/lists/*

  pip3 install --upgrade pip
  pip3 install "numpy<2.0"

  export PATH=/usr/local/cuda/bin:$PATH
  export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
  export CUDA_HOME=/usr/local/cuda

  cd /opt
  mkdir /opt/cmake
  wget https://github.com/Kitware/CMake/releases/download/v3.29.2/cmake-3.29.2-linux-aarch64.sh
  chmod +x cmake-3.29.2-linux-aarch64.sh
  ./cmake-3.29.2-linux-aarch64.sh --prefix=/opt/cmake --skip-license
  export PATH=/opt/cmake/bin:$PATH
  cmake --version
  rm cmake-3.29.2-linux-aarch64.sh

  cd /opt
  git clone https://github.com/gflags/gflags.git
  cd gflags
  cmake -B build -DCMAKE_INSTALL_PREFIX=/usr/local
  cd build
  make -j32
  make install
  cd /opt
  rm -rf gflags

  cd /opt
  git clone https://github.com/ggerganov/llama.cpp.git
  cd llama.cpp
  cmake -B build -DGGML_CUDA=ON -DGGML_CUDA_ENABLE_UNIFIED_MEMORY=1 -DGGML_CUDA_FORCE_CUBLAS=true -DGGML_CUDA_F16=true -DLLAMA_CURL=ON
  cd build
  cmake --build . --verbose -j32
  cmake --install .
  cd /opt
  rm -rf llama.cpp

  cd /opt
  git clone https://github.com/facebookresearch/faiss.git
  cd faiss
  cmake -B build .
  make -C build -j faiss
  make -C build -j swigfaiss
  (cd build/faiss/python && python3 setup.py install)
  make -C build install

  pip3 install langchain sentence-transformers
  pip3 install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu123
  pip3 install -U langchain-community
  pip3 install -U langchain-huggingface

%environment
  export PATH=/usr/local/cuda/bin:/opt/uv/bin:/usr/local/go/bin:/opt/cmake/bin:$PATH
  export LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/lib:$LD_LIBRARY_PATH
  export NVIDIA_VISIBLE_DEVICES=all
  export NVIDIA_DRIVER_CAPABILITIES=compute,utility
